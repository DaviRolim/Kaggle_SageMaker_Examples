{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "wicked-youth",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/c/tabular-playground-series-apr-2021/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fossil-directory",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "desperate-cathedral",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('figure', figsize=(12.0, 6.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "junior-helping",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "# Importing data and merging\n",
    "####################################\n",
    "\n",
    "# Reading dataset\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "ids = test['PassengerId']\n",
    "# Adding a column in each dataset before merging\n",
    "train['Type'] = 'train'\n",
    "test['Type'] = 'test'\n",
    "\n",
    "# Merging train and test\n",
    "data = train.append(test)\n",
    "\n",
    "####################################\n",
    "# Missing values and new features\n",
    "####################################\n",
    "    \n",
    "# New feature : Family_size\n",
    "data['Family_Size'] = data['Parch'] + data['SibSp'] + 1\n",
    "data.loc[:,'FsizeD'] = 'Alone'\n",
    "data.loc[(data['Family_Size'] > 1),'FsizeD'] = 'Small'\n",
    "data.loc[(data['Family_Size'] > 4),'FsizeD'] = 'Big'\n",
    "\n",
    "# Replacing missing Fare by median/Pclass \n",
    "fa = data[data[\"Pclass\"] == 3]\n",
    "data['Fare'].fillna(fa['Fare'].median(), inplace = True)\n",
    "\n",
    "#  New feature : Child\n",
    "data.loc[:,'Child'] = 1\n",
    "data.loc[(data['Age'] >= 18),'Child'] =0\n",
    "\n",
    "# New feature : Family Survival \n",
    "# data['Last_Name'] = data['Name'].apply(lambda x: str.split(x, \",\")[0])\n",
    "# DEFAULT_SURVIVAL_VALUE = 0.5\n",
    "\n",
    "# data['Family_Survival'] = DEFAULT_SURVIVAL_VALUE\n",
    "# for grp, grp_df in data[['Survived','Fare', 'Ticket', 'PassengerId',\n",
    "#                            'SibSp', 'Parch', 'Age', 'Cabin']].groupby(['Last_Name', 'Fare']):\n",
    "                               \n",
    "#     if (len(grp_df) != 1):\n",
    "#         # A Family group is found.\n",
    "#         for ind, row in grp_df.iterrows():\n",
    "#             smax = grp_df.drop(ind)['Survived'].max()\n",
    "#             smin = grp_df.drop(ind)['Survived'].min()\n",
    "#             passID = row['PassengerId']\n",
    "#             if (smax == 1.0):\n",
    "#                 data.loc[data['PassengerId'] == passID, 'Family_Survival'] = 1\n",
    "#             elif (smin == 0.0):\n",
    "#                 data.loc[data['PassengerId'] == passID, 'Family_Survival'] = 0\n",
    "                \n",
    "# for _, grp_df in data.groupby('Ticket'):\n",
    "#     if (len(grp_df) != 1):\n",
    "#         for ind, row in grp_df.iterrows():\n",
    "#             if (row['Family_Survival'] == 0) | (row['Family_Survival']== 0.5):\n",
    "#                 smax = grp_df.drop(ind)['Survived'].max()\n",
    "#                 smin = grp_df.drop(ind)['Survived'].min()\n",
    "#                 passID = row['PassengerId']\n",
    "#                 if (smax == 1.0):\n",
    "#                     data.loc[data['PassengerId'] == passID, 'Family_Survival'] = 1\n",
    "#                 elif (smin == 0.0):\n",
    "#                     data.loc[data['PassengerId'] == passID, 'Family_Survival'] = 0\n",
    "                    \n",
    "####################################\n",
    "# Encoding and pre-modeling\n",
    "####################################                  \n",
    "\n",
    "# dropping useless features\n",
    "data = data.drop(columns = ['Age','Cabin','Embarked', 'Name',\n",
    "                            'Parch', 'SibSp','Ticket', 'Family_Size'])\n",
    "\n",
    "# Encoding features\n",
    "target_col = [\"Survived\"]\n",
    "id_dataset = [\"Type\"]\n",
    "cat_cols   = data.nunique()[data.nunique() < 12].keys().tolist()\n",
    "cat_cols   = [x for x in cat_cols ]\n",
    "# numerical columns\n",
    "num_cols   = [x for x in data.columns if x not in cat_cols + target_col + id_dataset]\n",
    "# Binary columns with 2 values\n",
    "bin_cols   = data.nunique()[data.nunique() == 2].keys().tolist()\n",
    "# Columns more than 2 values\n",
    "multi_cols = [i for i in cat_cols if i not in bin_cols]\n",
    "# Label encoding Binary columns\n",
    "le = LabelEncoder()\n",
    "for i in bin_cols :\n",
    "    data[i] = le.fit_transform(data[i])\n",
    "# Duplicating columns for multi value columns\n",
    "data = pd.get_dummies(data = data,columns = multi_cols )\n",
    "# Scaling Numerical columns\n",
    "std = StandardScaler()\n",
    "scaled = std.fit_transform(data[num_cols])\n",
    "scaled = pd.DataFrame(scaled,columns = num_cols)\n",
    "# dropping original values merging scaled values for numerical columns\n",
    "df_data_og = data.copy()\n",
    "data = data.drop(columns = num_cols,axis = 1)\n",
    "data = data.merge(scaled,left_index = True,right_index = True,how = \"left\")\n",
    "data = data.drop(columns = ['PassengerId'],axis = 1)\n",
    "\n",
    "# Target = 1st column\n",
    "cols = data.columns.tolist()\n",
    "cols.insert(0, cols.pop(cols.index('Survived')))\n",
    "data = data.reindex(columns= cols)\n",
    "\n",
    "# Cutting train and test\n",
    "train = data[data['Type'] == 1].drop(columns = ['Type'])\n",
    "test = data[data['Type'] == 0].drop(columns = ['Type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "upper-rebecca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100000, 10), (100000, 10))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "amended-qualification",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isna().any().sum(), test.isna().any().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "located-patrick",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Child</th>\n",
       "      <th>Pclass_1</th>\n",
       "      <th>Pclass_2</th>\n",
       "      <th>Pclass_3</th>\n",
       "      <th>FsizeD_Alone</th>\n",
       "      <th>FsizeD_Big</th>\n",
       "      <th>FsizeD_Small</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.259156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.463749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.395870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.468349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.546685</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Sex  Child  Pclass_1  Pclass_2  Pclass_3  FsizeD_Alone  \\\n",
       "0         1    1      1         1         0         0             0   \n",
       "1         0    1      1         0         0         1             1   \n",
       "2         0    1      1         0         0         1             0   \n",
       "3         0    1      0         0         0         1             1   \n",
       "4         1    1      0         0         0         1             1   \n",
       "\n",
       "   FsizeD_Big  FsizeD_Small      Fare  \n",
       "0           0             1 -0.259156  \n",
       "1           0             0 -0.463749  \n",
       "2           0             1  0.395870  \n",
       "3           0             0 -0.468349  \n",
       "4           0             0 -0.546685  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automotive-technology",
   "metadata": {},
   "source": [
    "pretty neat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demanding-allowance",
   "metadata": {},
   "source": [
    "## Predicting using simple classification models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "covered-amount",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, StackingClassifier, VotingClassifier, ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "herbal-channels",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train.drop('Survived', axis=1), train.Survived, test_size=0.2, random_state=0, stratify=train.Survived)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "potential-plenty",
   "metadata": {},
   "source": [
    "### Simple function to train and display metrics of generalised models using GridSearch to find good parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "constant-textbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FitModel(algorithm,gridSearchParams,cv):\n",
    "    grid = GridSearchCV(\n",
    "        estimator=algorithm,\n",
    "        param_grid=gridSearchParams,\n",
    "        cv=cv,  verbose=1, n_jobs=-1)\n",
    "    \n",
    "    grid_result = grid.fit(X_train, y_train)\n",
    "    best_params = grid_result.best_params_\n",
    "    \n",
    "    print('Best Params :',best_params)\n",
    "    \n",
    "    pred = grid.predict(X_test) \n",
    "    print(accuracy_score(y_test, pred))\n",
    "    print(classification_report(y_test, pred))\n",
    "    print(confusion_matrix(y_test, pred))\n",
    "    \n",
    "    return grid_result # returning the model\n",
    "\n",
    "pd.options.display.float_format = '{:.2f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instant-screening",
   "metadata": {},
   "source": [
    "### Starting with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "failing-butterfly",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Best Params : {}\n",
      "0.75935\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.78      0.79     11445\n",
      "           1       0.71      0.73      0.72      8555\n",
      "\n",
      "    accuracy                           0.76     20000\n",
      "   macro avg       0.75      0.76      0.75     20000\n",
      "weighted avg       0.76      0.76      0.76     20000\n",
      "\n",
      "[[8941 2504]\n",
      " [2309 6246]]\n"
     ]
    }
   ],
   "source": [
    "lr = FitModel(LogisticRegression(), {}, 5)\n",
    "#76"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "former-guinea",
   "metadata": {},
   "source": [
    "## AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "opening-steam",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Best Params : {'learning_rate': 0.1, 'n_estimators': 30}\n",
      "0.75764\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.78      0.79     14307\n",
      "           1       0.71      0.73      0.72     10693\n",
      "\n",
      "    accuracy                           0.76     25000\n",
      "   macro avg       0.75      0.75      0.75     25000\n",
      "weighted avg       0.76      0.76      0.76     25000\n",
      "\n",
      "[[11098  3209]\n",
      " [ 2850  7843]]\n",
      "CPU times: user 1.76 s, sys: 68 ms, total: 1.83 s\n",
      "Wall time: 2min 51s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=AdaBoostClassifier(), n_jobs=-1,\n",
       "             param_grid={'learning_rate': [0.1, 0.5, 1.0],\n",
       "                         'n_estimators': [30, 50, 100, 500]},\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "params = {'n_estimators': [30,50,100,500], 'learning_rate': [0.1,0.5,1.0]}\n",
    "ada = FitModel(AdaBoostClassifier(), params, 5)\n",
    "#75 Best Params : {'learning_rate': 0.1, 'n_estimators': 30}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ahead-personal",
   "metadata": {},
   "source": [
    "## RandomForestClassifier\n",
    "StackingClassifier, VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "forward-fiction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Best Params : {'n_estimators': 500}\n",
      "0.68852\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.74      0.73     14307\n",
      "           1       0.64      0.62      0.63     10693\n",
      "\n",
      "    accuracy                           0.69     25000\n",
      "   macro avg       0.68      0.68      0.68     25000\n",
      "weighted avg       0.69      0.69      0.69     25000\n",
      "\n",
      "[[10614  3693]\n",
      " [ 4094  6599]]\n",
      "CPU times: user 54 s, sys: 540 ms, total: 54.5 s\n",
      "Wall time: 3min 36s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=RandomForestClassifier(), n_jobs=-1,\n",
       "             param_grid={'n_estimators': [30, 50, 100, 500]}, verbose=1)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "params = {'n_estimators': [30,50,100,500]}\n",
    "rf = FitModel(RandomForestClassifier(), params, 5)\n",
    "#69 Best Params : {'n_estimators': 500}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worldwide-methodology",
   "metadata": {},
   "source": [
    "## XGBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bronze-services",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "equivalent-ordinance",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 12 candidates, totalling 48 fits\n",
      "[19:45:55] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Best Params : {'colsample_bytree': 0.5, 'gamma': 0.3, 'learning_rate': 0.1, 'max_depth': 5, 'min_child_weight': 3}\n",
      "0.76204\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.77      0.79     14307\n",
      "           1       0.71      0.75      0.73     10693\n",
      "\n",
      "    accuracy                           0.76     25000\n",
      "   macro avg       0.76      0.76      0.76     25000\n",
      "weighted avg       0.76      0.76      0.76     25000\n",
      "\n",
      "[[11067  3240]\n",
      " [ 2709  7984]]\n",
      "CPU times: user 3.48 s, sys: 28.1 ms, total: 3.51 s\n",
      "Wall time: 3min 20s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=4,\n",
       "             estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                     colsample_bylevel=None,\n",
       "                                     colsample_bynode=None,\n",
       "                                     colsample_bytree=None, gamma=None,\n",
       "                                     gpu_id=None, importance_type='gain',\n",
       "                                     interaction_constraints=None,\n",
       "                                     learning_rate=None, max_delta_step=None,\n",
       "                                     max_depth=None, min_child_weight=None,\n",
       "                                     missing=nan, monotone_constraints=None,\n",
       "                                     n_estimators=100, n_jobs=None,\n",
       "                                     num_parallel_tree=None, random_state=None,\n",
       "                                     reg_alpha=None, reg_lambda=None,\n",
       "                                     scale_pos_weight=None, subsample=None,\n",
       "                                     tree_method=None, validate_parameters=None,\n",
       "                                     verbosity=None),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'colsample_bytree': [0.5],\n",
       "                         'gamma': [0.1, 0.2, 0.3, 0.4], 'learning_rate': [0.1],\n",
       "                         'max_depth': [5, 6, 10], 'min_child_weight': [3]},\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "params={\"learning_rate\"    : [0.1] ,\n",
    " \"max_depth\"        : [5],\n",
    " \"min_child_weight\" : [3],\n",
    " \"gamma\"            : [0.3],\n",
    " \"colsample_bytree\" : [0.5] }\n",
    "xgb = FitModel(XGBClassifier(), params, 4)\n",
    "# 0.76 Best Params : {'colsample_bytree': 0.5, 'gamma': 0.3, 'learning_rate': 0.1, 'max_depth': 5, 'min_child_weight': 3}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boring-behalf",
   "metadata": {},
   "source": [
    "## VotingClassifier + StackingClassifier + First submissions (for each model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "friendly-mixture",
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_clf = AdaBoostClassifier(n_estimators=30, learning_rate=0.1)\n",
    "lr_clf = LogisticRegression()\n",
    "rf_clf = RandomForestClassifier()\n",
    "xgb_clf = XGBClassifier(learning_rate=0.1, max_depth=5, min_child_weight=3, gamma=0.3, colsample_bytree=0.5)\n",
    "estimators = [('lr',lr_clf),('ada',ada_clf),('rf',rf_clf),('xgb',xgb_clf)]\n",
    "voting_clf = VotingClassifier(estimators=estimators)\n",
    "stacking_clf = StackingClassifier(estimators=estimators, final_estimator=ExtraTreesClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "eastern-supplier",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_submission(predictions, csv_name):\n",
    "    df = pd.DataFrame(columns=['PassengerId','Survived'])\n",
    "    df['PassengerId'] = ids\n",
    "    df['Survived'] = pd.Series(predictions)\n",
    "    df.to_csv(csv_name, header=True, index=False)\n",
    "    return df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "periodic-intellectual",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:07:15] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:08:07] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:08:08] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:08:10] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:08:11] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[21:08:12] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "StackingClassifier 0.73945\n"
     ]
    }
   ],
   "source": [
    "for name, clf in estimators+[('voting',voting_clf), ('stacking',stacking_clf)]:\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__,\n",
    "          accuracy_score(y_test, y_pred))\n",
    "    y_pred = clf.predict(test.drop('Survived', axis=1))\n",
    "    generate_submission(y_pred, f'{name}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prompt-generic",
   "metadata": {},
   "source": [
    "## KerasClassifier (ROOM FOR IMPROVEMENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cleared-colony",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "alien-mixture",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    dim=X_train.shape[1]\n",
    "    model = Sequential()\n",
    "    model.add(Dense(dim, activation='relu'))\n",
    "    \n",
    "    model.add(Dense(dim/2, activation='relu'))\n",
    "    \n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    \n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "oriental-efficiency",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "injured-hampshire",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((80000, 9), (80000,), (20000, 9), (20000,))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "verbal-junction",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: 0.6178 - accuracy: 0.6424 - val_loss: 0.5315 - val_accuracy: 0.7606\n",
      "Epoch 2/40\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: 0.5713 - accuracy: 0.7050 - val_loss: 0.5279 - val_accuracy: 0.7587\n",
      "Epoch 3/40\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: 0.5637 - accuracy: 0.7125 - val_loss: 0.5258 - val_accuracy: 0.7615\n",
      "Epoch 4/40\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: 0.5675 - accuracy: 0.7119 - val_loss: 0.5259 - val_accuracy: 0.7599\n",
      "Epoch 5/40\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: 0.5673 - accuracy: 0.7089 - val_loss: 0.5195 - val_accuracy: 0.7613\n",
      "Epoch 6/40\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: 0.5645 - accuracy: 0.7113 - val_loss: 0.5226 - val_accuracy: 0.7577\n",
      "Epoch 7/40\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: 0.5640 - accuracy: 0.7127 - val_loss: 0.5269 - val_accuracy: 0.7584\n",
      "Epoch 8/40\n",
      "2500/2500 [==============================] - 3s 1ms/step - loss: 0.5612 - accuracy: 0.7134 - val_loss: 0.5217 - val_accuracy: 0.7520\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8cc0e0b668>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=40, validation_data=(X_test,y_test), callbacks=[EarlyStopping(patience=4, monitor='accuracy')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "divine-hollywood",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(test.drop('Survived', axis=1)).flatten()\n",
    "pred = pred>0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "removable-ghost",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100002</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived\n",
       "0       100000         0\n",
       "1       100001         0\n",
       "2       100002         1"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = pred.astype(int)\n",
    "generate_submission(pred, 'keras.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "consecutive-gazette",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/ec2-user/.kaggle/kaggle.json'\n",
      "100%|████████████████████████████████████████| 879k/879k [00:00<00:00, 1.91MB/s]\n",
      "Successfully submitted to Tabular Playground Series - Apr 2021"
     ]
    }
   ],
   "source": [
    "!kaggle competitions submit -c tabular-playground-series-apr-2021 -f keras.csv -m \"Keras 2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compatible-dutch",
   "metadata": {},
   "source": [
    "## SageMaker XGBoost + Hiperparameter tunning job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "miniature-possibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.xgboost.estimator import XGBoost\n",
    "from sagemaker import image_uris\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker import get_execution_role\n",
    "import boto3\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "session = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "bucket = session.default_bucket()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grand-singles",
   "metadata": {},
   "source": [
    "## SageMaker Autopilot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "registered-trail",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
